{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d2c46fe",
   "metadata": {},
   "source": [
    "Based on the code shown in https://scikit-image.org/docs/stable/auto_examples/features_detection/plot_sift.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3d271516",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from skimage.color import rgb2gray\n",
    "from skimage.feature import SIFT, match_descriptors, plot_matched_features\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "from skimage.measure import ransac\n",
    "from skimage.transform import ProjectiveTransform\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e0368377",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the datafiles and make them grayscale\n",
    "def load_image(path):\n",
    "    return rgb2gray(np.array(Image.open(path)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "68e758bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R√©pertoire courant : c:\\Users\\emmar\\OneDrive\\Polytech\\I5\\Semestre 9 - Oslo\\Data Mining\\Exercices\\Day 5\\Augmented_reality_system\\src\n"
     ]
    }
   ],
   "source": [
    "# Used to acces the name of the folder we are working on : not necessary\n",
    "print(\"R√©pertoire courant :\", os.getcwd())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "857fa75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract SIFT keypoints and descriptors\n",
    "def extract_sift_features(image):\n",
    "    sift = SIFT()\n",
    "    sift.detect_and_extract(image)\n",
    "    return sift.keypoints, sift.descriptors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728a84fd",
   "metadata": {},
   "source": [
    "Problem of dimensions for the first two pictures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "db9c9955",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_to_match(img1, img2):\n",
    "    h1, w1 = img1.shape[:2]\n",
    "    h2, w2 = img2.shape[:2]\n",
    "    return cv2.resize(img2, (w1, h1)) if (h1 != h2 or w1 != w2) else img2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d45e0e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the matches\n",
    "def match_and_plot(img1, img2, keypoints1, descriptors1, keypoints2, descriptors2, title):\n",
    "    matches = match_descriptors(descriptors1, descriptors2, max_ratio=0.6, cross_check=True)\n",
    "\n",
    "    #img2 = resize_to_match(img1, img2)\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 1, figsize=(10, 8))\n",
    "    plt.gray()\n",
    "    plot_matched_features(image0=img1, image1=img2, \n",
    "                          keypoints0=keypoints1, keypoints1=keypoints2,\n",
    "                          matches=matches, ax=ax, \n",
    "                          keypoints_color='cyan',\n",
    "                          only_matches=True)\n",
    "    ax.axis('off')\n",
    "    ax.set_title(title)\n",
    "    plt.show()\n",
    "    return matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f104c122",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute matching accuracy using optional homography\n",
    "def compute_matching_accuracy(keypoints1, keypoints2, matches, homography=None, threshold=5):\n",
    "    \"\"\"\n",
    "    Compute the matching accuracy based on a homography.\n",
    "    If no homography is provided, only the number of matches is printed.\n",
    "    \"\"\"\n",
    "    if homography is None:\n",
    "        print(f\"{len(matches)} matches found (no validation)\")\n",
    "        return None\n",
    "\n",
    "    if matches is None or len(matches) == 0:\n",
    "        print(\"No matches to evaluate.\")\n",
    "        return 0\n",
    "\n",
    "    correct = 0\n",
    "    for i, j in matches:\n",
    "        pt1 = np.array([*keypoints1[i], 1.0])  # homogeneous coordinates\n",
    "        projected = homography @ pt1\n",
    "        projected /= projected[2]  # normalize\n",
    "\n",
    "        pt2 = keypoints2[j]\n",
    "        error = np.linalg.norm(projected[:2] - pt2)\n",
    "\n",
    "        if error < threshold:\n",
    "            correct += 1\n",
    "\n",
    "    accuracy = correct / len(matches) if matches.size > 0 else 0\n",
    "    print(f\"Matching Accuracy: {accuracy:.2%} ({correct}/{len(matches)} correct matches)\")\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6aff216",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_patch(img, keypoint, size=21):\n",
    "    x, y = int(keypoint[0]), int(keypoint[1])\n",
    "    half = size // 2\n",
    "    patch = img[max(0, y - half):y + half + 1, max(0, x - half):x + half + 1]\n",
    "    if patch.shape != (size, size):\n",
    "        return None  # Ignore les bords\n",
    "    return patch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2a65d998",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalized_cross_correlation(patch1, patch2):\n",
    "    \"\"\"\n",
    "    Calcule la corr√©lation crois√©e normalis√©e entre deux patchs d'image.\n",
    "    \n",
    "    Args:\n",
    "        patch1 (np.ndarray): Premier patch (grayscale, 2D array).\n",
    "        patch2 (np.ndarray): Deuxi√®me patch (m√™me taille que patch1).\n",
    "    \n",
    "    Returns:\n",
    "        float: Score de corr√©lation crois√©e normalis√©e (entre -1 et 1).\n",
    "    \"\"\"\n",
    "    if patch1.shape != patch2.shape:\n",
    "        raise ValueError(\"Les deux patchs doivent avoir la m√™me taille.\")\n",
    "\n",
    "    # Centrage des patchs\n",
    "    patch1_mean = patch1 - np.mean(patch1)\n",
    "    patch2_mean = patch2 - np.mean(patch2)\n",
    "\n",
    "    # Calcul du score NCC\n",
    "    numerator = np.sum(patch1_mean * patch2_mean)\n",
    "    denominator = np.sqrt(np.sum(patch1_mean**2) * np.sum(patch2_mean**2))\n",
    "\n",
    "    if denominator == 0:\n",
    "        return 0.0  # Cas d√©g√©n√©r√© : patch uniforme\n",
    "\n",
    "    return numerator / denominator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e0b38c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_by_ncc(img_ref, img_test, keypoints_ref, keypoints_test, threshold=0.8):\n",
    "    matches = []\n",
    "    for i, kp_test in enumerate(keypoints_test):\n",
    "        patch_test = extract_patch(img_test, kp_test)\n",
    "        if patch_test is None:\n",
    "            continue\n",
    "\n",
    "        best_score = -1\n",
    "        best_idx = -1\n",
    "        for j, kp_ref in enumerate(keypoints_ref):\n",
    "            patch_ref = extract_patch(img_ref, kp_ref)\n",
    "            if patch_ref is None:\n",
    "                continue\n",
    "\n",
    "            score = normalized_cross_correlation(patch_test, patch_ref)\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_idx = j\n",
    "\n",
    "        if best_score >= threshold:\n",
    "            matches.append(cv2.DMatch(_queryIdx=i, _trainIdx=best_idx, _distance=1 - best_score))\n",
    "    return matches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e298509b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_homography_from_matches(keypoints_ref, keypoints_test, matches, ransac_thresh=5.0):\n",
    "    \"\"\"\n",
    "    Estime l'homographie entre deux images √† partir des correspondances de points cl√©s.\n",
    "\n",
    "    Args:\n",
    "        keypoints_ref (list of cv2.KeyPoint): Points cl√©s de l'image de r√©f√©rence.\n",
    "        keypoints_test (list of cv2.KeyPoint): Points cl√©s de l'image test.\n",
    "        matches (list of cv2.DMatch): Correspondances entre les points.\n",
    "        ransac_thresh (float): Seuil RANSAC pour l'estimation robuste.\n",
    "\n",
    "    Returns:\n",
    "        H (np.ndarray): Matrice d'homographie 3x3 ou None si estimation √©choue.\n",
    "    \"\"\"\n",
    "    if len(matches) < 4:\n",
    "        return None  # Pas assez de points pour estimer une homographie\n",
    "\n",
    "    pts_ref = np.float32([keypoints_ref[m.trainIdx] for m in matches])\n",
    "    pts_test = np.float32([keypoints_test[m.queryIdx] for m in matches])\n",
    "\n",
    "    H, mask = cv2.findHomography(pts_ref, pts_test, cv2.RANSAC, ransac_thresh)\n",
    "    return H\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0be928bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset_index(dataset_dir):\n",
    "    \"\"\"\n",
    "    Analyse toutes les images du dataset et stocke les informations cl√©s dans un DataFrame.\n",
    "\n",
    "    Args:\n",
    "        dataset_dir (str or Path): Dossier contenant les images de r√©f√©rence.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame contenant les informations extraites pour chaque image.\n",
    "    \"\"\"\n",
    "    dataset_paths = sorted(Path(dataset_dir).rglob(\"*\"))\n",
    "    records = []\n",
    "\n",
    "    for ref_path in dataset_paths:\n",
    "        if ref_path.suffix.lower() not in [\".jpg\", \".jpeg\", \".png\", \".avif\"]:\n",
    "            continue\n",
    "\n",
    "        img_ref = load_image(str(ref_path))\n",
    "        keypoints_ref, descriptors_ref = extract_sift_features(img_ref)\n",
    "\n",
    "        if len(keypoints_ref) == 0:\n",
    "            print(f\"‚ö†Ô∏è Skipped {ref_path.name} due to missing keypoints\")\n",
    "            continue\n",
    "\n",
    "        record = {\n",
    "            \"filename\": ref_path.name,\n",
    "            \"path\": str(ref_path),\n",
    "            \"num_keypoints\": len(keypoints_ref),\n",
    "            \"keypoints\": keypoints_ref,\n",
    "            \"descriptors\": descriptors_ref,\n",
    "            \"image_shape\": img_ref.shape\n",
    "        }\n",
    "        records.append(record)\n",
    "\n",
    "    df = pd.DataFrame(records)\n",
    "    return df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "022881f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def batch_compare_tests_to_dataset(test_dir, dataset_dir, ncc_threshold=0.7):\n",
    "    dataset_df = build_dataset_index(dataset_dir)\n",
    "    \n",
    "    for test_path in sorted(Path(test_dir).glob(\"*\")):\n",
    "        print(f\"\\nüß™ Test image: {test_path.name}\")\n",
    "        img_test = load_image(str(test_path))\n",
    "        keypoints_test, _ = extract_sift_features(img_test)\n",
    "\n",
    "        for _, row in dataset_df.iterrows():\n",
    "            keypoints_ref = row[\"keypoints\"]\n",
    "            img_ref = load_image(row[\"path\"])\n",
    "\n",
    "            if len(keypoints_ref) == 0 or len(keypoints_test) == 0:\n",
    "                print(f\"‚ö†Ô∏è Skipped {row['filename']} due to missing keypoints\")\n",
    "                continue\n",
    "\n",
    "            matches = match_by_ncc(img_ref, img_test, keypoints_ref, keypoints_test, threshold=ncc_threshold)\n",
    "            if not matches:\n",
    "                print(f\"‚ùå No NCC matches between {row['filename']} and {test_path.name}\")\n",
    "                continue\n",
    "\n",
    "            H = estimate_homography_from_matches(keypoints_ref, keypoints_test, matches)\n",
    "            accuracy = compute_matching_accuracy(keypoints_ref, keypoints_test, matches, H)\n",
    "\n",
    "            print(f\"üîó Accuracy of {row['filename']} vs {test_path.name}: {accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131c99b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üß™ Test image: city_hall_test.jpg\n"
     ]
    }
   ],
   "source": [
    "batch_compare_tests_to_dataset(\"../data/test\", \"../data/images\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
